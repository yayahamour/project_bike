{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDClassifier, Lasso,RidgeCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve, KFold\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_absolute_percentage_error, RocCurveDisplay, mean_squared_log_error,  accuracy_score,precision_score,recall_score,f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, RobustScaler, StandardScaler, FunctionTransformer, LabelEncoder, OneHotEncoder, Binarizer, OrdinalEncoder, MaxAbsScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, RFECV \n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor, ExtraTreesRegressor, VotingClassifier, VotingRegressor,StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# from pycaret.regression import *\n",
    "import xgboost\n",
    "from os import stat\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from contextlib import closing\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from datetime import datetime \n",
    "import lightgbm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather(x):\n",
    "    if x == 1:\n",
    "        x = 4\n",
    "    elif x == 2:\n",
    "        x = 3\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_season(x):\n",
    "    if x == 1:\n",
    "        x = 3\n",
    "    elif x == 2:\n",
    "        x = 4\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all =  pd.read_csv('../data/train.csv')\n",
    "train_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-17 08:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 75, 'atemp' : 72, 'humidity' : 2, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-07-15 09:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : 96, 'atemp' : 88, 'humidity' : 2, 'windspeed' : 500.0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-10 11:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 51, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-12 19:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 4, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-08 18:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 9, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-24 17:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -110, 'atemp' : -108, 'humidity' : 25, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-25 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 69, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : -500, 'atemp' : -55, 'humidity' : 0, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-07-21 15:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 2, 'temp' : 75, 'atemp' : 72.5, 'humidity' : 41, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-04 13:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 55, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-05 14:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 25, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-28 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 39, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 09:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : -110, 'atemp' : -108, 'humidity' : 10, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-27 12:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 57, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-01-02 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -500, 'atemp' : -55, 'humidity' : 84, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "\n",
    "train_data_all['day'] = pd.to_datetime(train_data_all['datetime']).dt.dayofyear\n",
    "train_data_all['hour'] = pd.to_datetime(train_data_all['datetime']).dt.hour\n",
    "train_data_all['year'] = pd.to_datetime(train_data_all['datetime']).dt.year\n",
    "\n",
    "train_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "for i in range(0, len(train_data_all.iloc[:,0])) :\n",
    "    train_data_all.iloc[i,0] = transform_season(int(train_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(train_data_all.iloc[:,3])) :\n",
    "    train_data_all.iloc[i,3] = transform_weather(int(train_data_all.iloc[i,3] ))\n",
    "\n",
    "\n",
    "train_data_all = train_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "augmentation = np.linspace(0, (144 * 100 / 230) *  train_data_all.shape[0] , train_data_all.shape[0])\n",
    "\n",
    "# train_data_all['augmentation'] = 0 \n",
    "# train_data_all['augmentation'] = augmentation # Etape suivante : Il faut order par day avant de drop datetime pour que ce soit cohérent \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_all_X = train_data_all.drop('count', axis = 1)\n",
    "train_data_all_y = train_data_all['count']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data_all =  pd.read_csv('../data/test.csv')\n",
    "test_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data_all['day'] = pd.to_datetime(test_data_all['datetime']).dt.dayofyear\n",
    "test_data_all['hour'] = pd.to_datetime(test_data_all['datetime']).dt.hour\n",
    "test_data_all['year'] = pd.to_datetime(test_data_all['datetime']).dt.year\n",
    "test_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(0, len(test_data_all.iloc[:,0])) :\n",
    "    test_data_all.iloc[i,0] = transform_season(int(test_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(test_data_all.iloc[:,3])) :\n",
    "    test_data_all.iloc[i,3] = transform_weather(int(test_data_all.iloc[i,3] ))\n",
    "\n",
    "# augmentation = np.linspace(0, (144 * 100 / 230) *  test_data_all.shape[0] / 100 , test_data_all.shape[0])\n",
    "# test_data_all['augmentation'] = augmentation\n",
    "\n",
    "\n",
    "test_data_all = test_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "test_data_all_X = test_data_all.drop('count', axis = 1)\n",
    "test_data_all_y = test_data_all['count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.986720359083927\n",
      "test data 0.8967965326020695\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "model_xgboost = make_pipeline(preprocessor,  xgb)\n",
    "model_xgboost.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_xgboost.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_xgboost.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9614442590972704\n",
      "test 0.9190960274167893\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(missing_values=np.nan), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9521333672398784\n",
      "test 0.9051328597939277\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(  SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_2)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "train 0.9522756520038445\n",
      "test 0.9118832634446215\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model n2 lgbm : \n",
    "# Preprocessing et model lgbm :0.9118832634446215 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor,  model_lgbm_3)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9588957920387866\n",
      "test 0.9074659738499581\n"
     ]
    }
   ],
   "source": [
    "# colsample_bytree => Pourcentage de features utilisé pour chaque arbre réduit a 46% et resultat pas loin de 73% + modif learning rate \n",
    "# Preprocessing et model lgbm : 0.9064545094483808(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_4)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:11:18] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9140491767414458"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.917337454042215 (2s4)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_voting = make_pipeline(preprocessor, VC)\n",
    "\n",
    "model_voting.fit(train_data_all_X, train_data_all_y)\n",
    "model_voting.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:12:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:50] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:12:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "mean abs error 39.64159024921\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_3)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)\n",
    "y_pred = model_stack.predict(test_data_all_X)\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:42] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:44] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:45] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:48] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:49] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244984606061998"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:59:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:00:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244984606061998"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Try without log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models \n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "def objective(trial):\n",
    "    import xgboost as xgb \n",
    "    dtrain = xgb.DMatrix(train_data_all_X, label=train_data_all_y)\n",
    "    dtest = xgb.DMatrix(test_data_all_X, label=test_data_all_y)\n",
    "\n",
    "    param = {\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.0, 1.0),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-8, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 0, 50),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 2.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 2.0),\n",
    "        \"eval_metric\": \"mae\",\n",
    "        \"random_state\" : 2\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-mae\")\n",
    "    best_model = xgb.train(param, dtrain, evals=[(dtest, \"validation-mae\")], callbacks=[pruning_callback])\n",
    "    preds = best_model.predict(dtest)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = mean_absolute_error(test_data_all_y, pred_labels)\n",
    "    # accuracy = { 'r2_score' : round(r2_score(test_data_all_y, pred_labels), 4), 'mse' :  round(mean_squared_error(test_data_all_y, pred_labels ),4), 'median abs err' :  round(median_absolute_error(test_data_all_y, pred_labels),4), 'mean abs err' :  round(mean_absolute_error(test_data_all_y, pred_labels),4)}\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 20:00:07,433]\u001b[0m A new study created in memory with name: no-name-46ee2993-3e32-42a5-a21c-8bafea191daf\u001b[0m\n",
      "\u001b[33m[W 2022-04-07 20:00:07,512]\u001b[0m Trial 0 failed because of the following error: AssertionError('Dataset name should not contain `-`')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_78565/1576332932.py\", line 23, in objective\n",
      "    best_model = xgb.train(param, dtrain, evals=[(dtest, \"validation-mae\")], callbacks=[pruning_callback])\n",
      "  File \"/home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"/home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py\", line 82, in _train_internal\n",
      "    if callbacks.after_iteration(bst, i, dtrain, evals):\n",
      "  File \"/home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py\", line 433, in after_iteration\n",
      "    assert name.find('-') == -1, 'Dataset name should not contain `-`'\n",
      "AssertionError: Dataset name should not contain `-`\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Dataset name should not contain `-`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000016?line=0'>1</a>\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000016?line=1'>2</a>\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000016?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(study\u001b[39m.\u001b[39mbest_trial)\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=391'>392</a>\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=392'>393</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=393'>394</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=394'>395</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=395'>396</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=396'>397</a>\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=397'>398</a>\u001b[0m     )\n\u001b[0;32m--> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=399'>400</a>\u001b[0m _optimize(\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=400'>401</a>\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=401'>402</a>\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=402'>403</a>\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=403'>404</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=404'>405</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=405'>406</a>\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=406'>407</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=407'>408</a>\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=408'>409</a>\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/study.py?line=409'>410</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=63'>64</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=64'>65</a>\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=65'>66</a>\u001b[0m         _optimize_sequential(\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=66'>67</a>\u001b[0m             study,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=67'>68</a>\u001b[0m             func,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=68'>69</a>\u001b[0m             n_trials,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=69'>70</a>\u001b[0m             timeout,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=70'>71</a>\u001b[0m             catch,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=71'>72</a>\u001b[0m             callbacks,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=72'>73</a>\u001b[0m             gc_after_trial,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=73'>74</a>\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=74'>75</a>\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=75'>76</a>\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=76'>77</a>\u001b[0m         )\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=77'>78</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=78'>79</a>\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=159'>160</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=161'>162</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=162'>163</a>\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=163'>164</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=164'>165</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=260'>261</a>\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=262'>263</a>\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch):\n\u001b[0;32m--> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=263'>264</a>\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=264'>265</a>\u001b[0m \u001b[39mreturn\u001b[39;00m trial\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=209'>210</a>\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=211'>212</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=212'>213</a>\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=213'>214</a>\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=214'>215</a>\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/optuna/study/_optimize.py?line=215'>216</a>\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb Cell 16'\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000015?line=20'>21</a>\u001b[0m \u001b[39m# Add a callback for pruning.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000015?line=21'>22</a>\u001b[0m pruning_callback \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mintegration\u001b[39m.\u001b[39mXGBoostPruningCallback(trial, \u001b[39m\"\u001b[39m\u001b[39mvalidation-mae\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000015?line=22'>23</a>\u001b[0m best_model \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39;49mtrain(param, dtrain, evals\u001b[39m=\u001b[39;49m[(dtest, \u001b[39m\"\u001b[39;49m\u001b[39mvalidation-mae\u001b[39;49m\u001b[39m\"\u001b[39;49m)], callbacks\u001b[39m=\u001b[39;49m[pruning_callback])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000015?line=23'>24</a>\u001b[0m preds \u001b[39m=\u001b[39m best_model\u001b[39m.\u001b[39mpredict(dtest)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ayoub/Documents/projet/simplon/projet_bike/project_bike/model/finition_notebook.ipynb#ch0000015?line=24'>25</a>\u001b[0m pred_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrint(preds)\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py:188\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(params, dtrain, num_boost_round\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, evals\u001b[39m=\u001b[39m(), obj\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, feval\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=115'>116</a>\u001b[0m           maximize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, evals_result\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=116'>117</a>\u001b[0m           verbose_eval\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, xgb_model\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, callbacks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=117'>118</a>\u001b[0m     \u001b[39m# pylint: disable=too-many-statements,too-many-branches, attribute-defined-outside-init\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=118'>119</a>\u001b[0m     \u001b[39m\"\"\"Train a booster with given parameters.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=119'>120</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=120'>121</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=185'>186</a>\u001b[0m \u001b[39m    Booster : a trained booster model\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=186'>187</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=187'>188</a>\u001b[0m     bst \u001b[39m=\u001b[39m _train_internal(params, dtrain,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=188'>189</a>\u001b[0m                           num_boost_round\u001b[39m=\u001b[39;49mnum_boost_round,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=189'>190</a>\u001b[0m                           evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=190'>191</a>\u001b[0m                           obj\u001b[39m=\u001b[39;49mobj, feval\u001b[39m=\u001b[39;49mfeval,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=191'>192</a>\u001b[0m                           xgb_model\u001b[39m=\u001b[39;49mxgb_model, callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=192'>193</a>\u001b[0m                           verbose_eval\u001b[39m=\u001b[39;49mverbose_eval,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=193'>194</a>\u001b[0m                           evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=194'>195</a>\u001b[0m                           maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=195'>196</a>\u001b[0m                           early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds)\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=196'>197</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m bst\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py:82\u001b[0m, in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=79'>80</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=80'>81</a>\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m---> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=81'>82</a>\u001b[0m     \u001b[39mif\u001b[39;00m callbacks\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=82'>83</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/training.py?line=84'>85</a>\u001b[0m bst \u001b[39m=\u001b[39m callbacks\u001b[39m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m~/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py:433\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py?line=430'>431</a>\u001b[0m evals \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m evals \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m evals\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py?line=431'>432</a>\u001b[0m \u001b[39mfor\u001b[39;00m _, name \u001b[39min\u001b[39;00m evals:\n\u001b[0;32m--> <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py?line=432'>433</a>\u001b[0m     \u001b[39massert\u001b[39;00m name\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mDataset name should not contain `-`\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py?line=433'>434</a>\u001b[0m score \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval_set(evals, epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric)\n\u001b[1;32m    <a href='file:///home/ayoub/anaconda3/envs/envIA/lib/python3.10/site-packages/xgboost/callback.py?line=434'>435</a>\u001b[0m score \u001b[39m=\u001b[39m score\u001b[39m.\u001b[39msplit()[\u001b[39m1\u001b[39m:]  \u001b[39m# into datasets\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset name should not contain `-`"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean abs err : 254.54854635216677\n",
      "best params : {'booster': 'dart', 'colsample_bytree': 0.6694010125888245, 'eta': 4.8781693920985254e-05, 'gamma': 0.1883994647052204, 'max_depth': 3, 'max_leaves': 23, 'alpha': 9.801925286079314e-07, 'lambda': 0.05802906174357909}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(f'mean abs err : {trial.value}')\n",
    "print('best params :', trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models \n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto', verbose=-1)\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7, verbose=-1)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56, verbose=-1)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56, verbose=-1)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7,  verbose=-1)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)],)\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model_2 = StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "\n",
    "liste_model = [xgb, model_lgbm, model_lgbm_2, model_lgbm_3, model_lgbm_4, VC, stack_model, stack_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\", \"verbose\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.986720359083927\n",
      "test data 0.8967965326020695\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=True, with_std=False), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_selected = make_pipeline(preprocessor,  xgb)\n",
    "model_selected.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_selected.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stacking\n",
      "r2 score train data 0.986720359083927\n",
      "r2 score test data 0.8967965326020695\n",
      "mean squared error 4793.1408218943025\n",
      "median abs error 32.69251251220703\n",
      "mean abs error 48.643447046910445\n"
     ]
    }
   ],
   "source": [
    "print(f'model stacking') \n",
    "\n",
    "\n",
    "print('r2 score train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('r2 score test data', model_selected.score(test_data_all_X, test_data_all_y))\n",
    "y_pred = model_selected.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ne pas touché ! Model saved avec le meilleur score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:21] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:14:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:14:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:14:25] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:14:27] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:14:28] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('maxabsscaler',\n",
       "                                                                   MaxAbsScaler())]),\n",
       "                                                  ['temp', 'atemp', 'humidity',\n",
       "                                                   'windspeed', 'day',\n",
       "                                                   'hour']),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler(with_mean=False)),\n",
       "                                                                  ('onehotencode...\n",
       "                                                             n_estimators=100,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             predictor=None,\n",
       "                                                             random_state=2,\n",
       "                                                             reg_alpha=0.22936408146810935,\n",
       "                                                             reg_lambda=0.03113496231279571,\n",
       "                                                             scale_pos_weight=None,\n",
       "                                                             subsample=None,\n",
       "                                                             subsample_freq=0.75,\n",
       "                                                             tree_method='auto',\n",
       "                                                             validate_parameters=None, ...)),\n",
       "                                               ('4',\n",
       "                                                LGBMRegressor(max_bin=1023,\n",
       "                                                              min_data_in_leaf=100,\n",
       "                                                              random_state=2,\n",
       "                                                              reg_alpha=0,\n",
       "                                                              subsample_freq=56))]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meilleur model : train data 0.9534144126106945\n",
    "#                  test data 0.9222952589076324 ou 92.24 avec StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "# All models \n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gblinear', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=2, \n",
    "                            reg_alpha=0.22936408146810935, reg_lambda = 0.03113496231279571, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model_2 = StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', model_lgbm_3)])\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(SimpleImputer(), StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_selected = make_pipeline(preprocessor,  stack_model_2)\n",
    "model_selected.fit(train_data_all_X , train_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model selectionnée\n",
      "r2 score train data 0.9534101810299771\n",
      "r2 score test data 0.9223019982104195\n",
      "mean squared error 3608.5751142574804\n",
      "median abs error 26.114137036951078\n",
      "mean abs error 40.7699687009164\n"
     ]
    }
   ],
   "source": [
    "print(f'model selectionnée') \n",
    "\n",
    "\n",
    "print('r2 score train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('r2 score test data', model_selected.score(test_data_all_X, test_data_all_y))\n",
    "y_pred = model_selected.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'stacking.pkl'\n",
    "pickle.dump(model_xgboost, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45357ead6f4da03ac2d24128fc463b8aec036f33d9539f6445fb7785aa61cc64"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('envIA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
