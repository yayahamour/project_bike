{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, Ridge, SGDClassifier, Lasso,RidgeCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, learning_curve, KFold\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_absolute_percentage_error, RocCurveDisplay, mean_squared_log_error,  accuracy_score,precision_score,recall_score,f1_score\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler, RobustScaler, StandardScaler, FunctionTransformer, LabelEncoder, OneHotEncoder, Binarizer, OrdinalEncoder, MaxAbsScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, RFECV \n",
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor, ExtraTreesRegressor, VotingClassifier, VotingRegressor,StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# from pycaret.regression import *\n",
    "import xgboost\n",
    "from os import stat\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from contextlib import closing\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from datetime import datetime \n",
    "import lightgbm\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_weather(x):\n",
    "    if x == 1:\n",
    "        x = 4\n",
    "    elif x == 2:\n",
    "        x = 3\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_season(x):\n",
    "    if x == 1:\n",
    "        x = 3\n",
    "    elif x == 2:\n",
    "        x = 4\n",
    "    elif x == 3:\n",
    "        x = 2 \n",
    "    elif x == 4:\n",
    "        x = 1\n",
    "        \n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all =  pd.read_csv('../data/train.csv')\n",
    "train_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-17 08:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 75, 'atemp' : 72, 'humidity' : 2, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-07-15 09:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : 96, 'atemp' : 88, 'humidity' : 2, 'windspeed' : 500.0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-10 11:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 51, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-08-12 19:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 4, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-08 18:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 9, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-24 17:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -110, 'atemp' : -108, 'humidity' : 25, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-25 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 69, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 1, 'temp' : -500, 'atemp' : -55, 'humidity' : 0, 'windspeed' : 110, 'casual' : 0, 'registered' : 0,  'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-07-21 15:00:00', 'season' : 0, 'holiday' : 0, 'workingday' : 0, 'weather' : 2, 'temp' : 75, 'atemp' : 72.5, 'humidity' : 41, 'windspeed' : 500, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-04 13:00:00', 'season' : 1, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : 200, 'atemp' : 155, 'humidity' : 55, 'windspeed' : 0, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-06-05 14:00:00', 'season' : 2, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : 111, 'atemp' : 110, 'humidity' : 25, 'windspeed' : 225, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-28 15:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 39, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-12-27 09:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 3, 'temp' : -110, 'atemp' : -108, 'humidity' : 10, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2011-11-27 12:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -48, 'atemp' : -55, 'humidity' : 57, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "train_data_all = train_data_all.append({'datetime' : '2012-01-02 16:00:00', 'season' : 4, 'holiday' : 0, 'workingday' : 0, 'weather' : 4, 'temp' : -500, 'atemp' : -55, 'humidity' : 84, 'windspeed' : 110, 'casual' : 0, 'registered' : 0, 'count' : 0}, ignore_index = True)\n",
    "\n",
    "train_data_all['day'] = pd.to_datetime(train_data_all['datetime']).dt.dayofyear\n",
    "train_data_all['hour'] = pd.to_datetime(train_data_all['datetime']).dt.hour\n",
    "train_data_all['year'] = pd.to_datetime(train_data_all['datetime']).dt.year\n",
    "\n",
    "train_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "for i in range(0, len(train_data_all.iloc[:,0])) :\n",
    "    train_data_all.iloc[i,0] = transform_season(int(train_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(train_data_all.iloc[:,3])) :\n",
    "    train_data_all.iloc[i,3] = transform_weather(int(train_data_all.iloc[i,3] ))\n",
    "\n",
    "\n",
    "train_data_all = train_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "augmentation = np.linspace(0, (144 * 100 / 230) *  train_data_all.shape[0] , train_data_all.shape[0])\n",
    "\n",
    "# train_data_all['augmentation'] = 0 \n",
    "# train_data_all['augmentation'] = augmentation # Etape suivante : Il faut order par day avant de drop datetime pour que ce soit cohérent \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data_all_X = train_data_all.drop('count', axis = 1)\n",
    "train_data_all_y = train_data_all['count']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_data_all =  pd.read_csv('../data/test.csv')\n",
    "test_data_all.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data_all['day'] = pd.to_datetime(test_data_all['datetime']).dt.dayofyear\n",
    "test_data_all['hour'] = pd.to_datetime(test_data_all['datetime']).dt.hour\n",
    "test_data_all['year'] = pd.to_datetime(test_data_all['datetime']).dt.year\n",
    "test_data_all.drop(['datetime'], axis=1, inplace=True)\n",
    "\n",
    "for i in range(0, len(test_data_all.iloc[:,0])) :\n",
    "    test_data_all.iloc[i,0] = transform_season(int(test_data_all.iloc[i,0] ))\n",
    "    \n",
    "for i in range(0, len(test_data_all.iloc[:,3])) :\n",
    "    test_data_all.iloc[i,3] = transform_weather(int(test_data_all.iloc[i,3] ))\n",
    "\n",
    "# augmentation = np.linspace(0, (144 * 100 / 230) *  test_data_all.shape[0] / 100 , test_data_all.shape[0])\n",
    "# test_data_all['augmentation'] = augmentation\n",
    "\n",
    "\n",
    "test_data_all = test_data_all.drop(['registered','casual'], axis=1)\n",
    "\n",
    "test_data_all_X = test_data_all.drop('count', axis = 1)\n",
    "test_data_all_y = test_data_all['count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:29:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.9866919813942476\n",
      "test data 0.9056841627368329\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "model_xgboost = make_pipeline(preprocessor,  xgb)\n",
    "model_xgboost.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_xgboost.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_xgboost.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9614442590972704\n",
      "test 0.9190960274167893\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(missing_values=np.nan), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9521333672398784\n",
      "test 0.9051328597939277\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing et model lgbm : 0.9190960274167893 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(  SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_2)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "train 0.9522756520038445\n",
      "test 0.9118832634446215\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model n2 lgbm : \n",
    "# Preprocessing et model lgbm :0.9118832634446215 (8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "model_lgbm_fit = make_pipeline(preprocessor,  model_lgbm_3)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.9588957920387866\n",
      "test 0.9074659738499581\n"
     ]
    }
   ],
   "source": [
    "# colsample_bytree => Pourcentage de features utilisé pour chaque arbre réduit a 46% et resultat pas loin de 73% + modif learning rate \n",
    "# Preprocessing et model lgbm : 0.9064545094483808(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "model_lgbm_fit = make_pipeline(preprocessor, model_lgbm_4)\n",
    "model_lgbm_fit.fit(train_data_all_X , train_data_all_y)\n",
    "print('train', model_lgbm_fit.score(train_data_all_X, train_data_all_y))\n",
    "print('test', model_lgbm_fit.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:45:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9140491767414458"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.917337454042215 (2s4)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(), OneHotEncoder())\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_voting = make_pipeline(preprocessor, VC)\n",
    "\n",
    "model_voting.fit(train_data_all_X, train_data_all_y)\n",
    "model_voting.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:45:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:45:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:45:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:01] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9288989999703046"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_3)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:46:03] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:05] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:06] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:07] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:46:08] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244984606061998"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:52:47] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:52:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:52:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:52:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:52:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[09:52:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9244984606061998"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing et model Votingmodel : 0.9288990012377096 (6s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline( logfunc, SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "model_stack = make_pipeline(preprocessor, stack_model)\n",
    "\n",
    "model_stack.fit(train_data_all_X, train_data_all_y)\n",
    "model_stack.score(test_data_all_X, test_data_all_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Try without log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models \n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "def objective(trial):\n",
    "    import xgboost as xgb \n",
    "    dtrain = xgb.DMatrix(train_data_all_X, label=train_data_all_y)\n",
    "    dtest = xgb.DMatrix(test_data_all_X, label=test_data_all_y)\n",
    "\n",
    "    param = {\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.0, 1.0),\n",
    "        \"eta\": trial.suggest_float(\"eta\", 1e-8, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 9),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 0, 50),\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 2.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 2.0),\n",
    "        \"eval_metric\": \"mae\",\n",
    "        \"random_state\" : 2\n",
    "    }\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-mae\")\n",
    "    best_model = xgb.train(param, dtrain, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n",
    "    preds = best_model.predict(dtest)\n",
    "    pred_labels = np.rint(preds)\n",
    "    accuracy = mean_absolute_error(test_data_all_y, pred_labels)\n",
    "    # accuracy = { 'r2_score' : round(r2_score(test_data_all_y, pred_labels), 4), 'mse' :  round(mean_squared_error(test_data_all_y, pred_labels ),4), 'median abs err' :  round(median_absolute_error(test_data_all_y, pred_labels),4), 'mean abs err' :  round(mean_absolute_error(test_data_all_y, pred_labels),4)}\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:15,094]\u001b[0m A new study created in memory with name: no-name-d94b8fcf-787b-4b2c-842d-98a691f8e959\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:183.81819\n",
      "[1]\tvalidation-mae:170.11801\n",
      "[2]\tvalidation-mae:165.63112\n",
      "[3]\tvalidation-mae:160.19601\n",
      "[4]\tvalidation-mae:154.32805\n",
      "[5]\tvalidation-mae:149.14421\n",
      "[6]\tvalidation-mae:145.33766\n",
      "[7]\tvalidation-mae:142.68709\n",
      "[8]\tvalidation-mae:141.01060\n",
      "[9]\tvalidation-mae:139.90868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:15,357]\u001b[0m Trial 0 finished with value: 139.90565002742733 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.716763990684362, 'eta': 0.7491884470343668, 'gamma': 0.8343035766534742, 'max_depth': 6, 'max_leaves': 18, 'alpha': 1.8144553459367666e-06, 'lambda': 4.196699545338333e-08}. Best is trial 0 with value: 139.90565002742733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:199.61871\n",
      "[1]\tvalidation-mae:176.94432\n",
      "[2]\tvalidation-mae:154.92331\n",
      "[3]\tvalidation-mae:148.00011\n",
      "[4]\tvalidation-mae:138.01090\n",
      "[5]\tvalidation-mae:131.76068\n",
      "[6]\tvalidation-mae:128.55708\n",
      "[7]\tvalidation-mae:123.81715\n",
      "[8]\tvalidation-mae:123.29951\n",
      "[9]\tvalidation-mae:121.41956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:15,558]\u001b[0m Trial 1 finished with value: 121.40811848601207 and parameters: {'booster': 'gbtree', 'colsample_bytree': 0.6406710256556656, 'eta': 0.3223858979902092, 'gamma': 0.9787970679525745, 'max_depth': 1, 'max_leaves': 5, 'alpha': 0.1371873011636385, 'lambda': 2.4155208835575777e-05}. Best is trial 0 with value: 139.90565002742733.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:165.15655\n",
      "[1]\tvalidation-mae:163.33344\n",
      "[2]\tvalidation-mae:160.43559\n",
      "[3]\tvalidation-mae:159.00974\n",
      "[4]\tvalidation-mae:157.48093\n",
      "[5]\tvalidation-mae:156.11151\n",
      "[6]\tvalidation-mae:154.83766\n",
      "[7]\tvalidation-mae:153.66992\n",
      "[8]\tvalidation-mae:152.58765\n",
      "[9]\tvalidation-mae:151.58705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:15,719]\u001b[0m Trial 2 finished with value: 151.59352715304442 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.7688852318624034, 'eta': 0.23418668508011917, 'gamma': 0.33449055749734913, 'max_depth': 8, 'max_leaves': 36, 'alpha': 2.435332085878695e-07, 'lambda': 0.16042852445342767}. Best is trial 2 with value: 151.59352715304442.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:15] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:165.41292\n",
      "[1]\tvalidation-mae:163.40533\n",
      "[2]\tvalidation-mae:160.36366\n",
      "[3]\tvalidation-mae:158.88913\n",
      "[4]\tvalidation-mae:157.31126\n",
      "[5]\tvalidation-mae:155.91075\n",
      "[6]\tvalidation-mae:154.61401\n",
      "[7]\tvalidation-mae:153.43936\n",
      "[8]\tvalidation-mae:152.34311\n",
      "[9]\tvalidation-mae:151.32520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:15,876]\u001b[0m Trial 3 finished with value: 151.32583653318704 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.5666830498826126, 'eta': 0.24270302359273557, 'gamma': 0.1554724962079976, 'max_depth': 4, 'max_leaves': 38, 'alpha': 2.04470377562312e-07, 'lambda': 0.0008580421115613002}. Best is trial 2 with value: 151.59352715304442.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:184.62941\n",
      "[1]\tvalidation-mae:154.28406\n",
      "[2]\tvalidation-mae:130.49147\n",
      "[3]\tvalidation-mae:115.63562\n",
      "[4]\tvalidation-mae:98.10077\n",
      "[5]\tvalidation-mae:94.35155\n",
      "[6]\tvalidation-mae:87.47385\n",
      "[7]\tvalidation-mae:86.19486\n",
      "[8]\tvalidation-mae:84.78221\n",
      "[9]\tvalidation-mae:82.34686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,081]\u001b[0m Trial 4 finished with value: 82.35600658255622 and parameters: {'booster': 'gbtree', 'colsample_bytree': 0.6303034143275993, 'eta': 0.42059585378065834, 'gamma': 0.7968668560131141, 'max_depth': 4, 'max_leaves': 2, 'alpha': 0.00022804586876499887, 'lambda': 1.1856199354749559e-06}. Best is trial 2 with value: 151.59352715304442.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:109.08105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,121]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:168.87695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,153]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:167.77487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,200]\u001b[0m Trial 7 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:16] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:159.23759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,258]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:16,306]\u001b[0m Trial 9 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:252.49672\n",
      "[1]\tvalidation-mae:249.94765\n",
      "[2]\tvalidation-mae:246.62476\n",
      "[3]\tvalidation-mae:244.40866\n",
      "[4]\tvalidation-mae:242.48824\n",
      "[5]\tvalidation-mae:240.52469\n",
      "[6]\tvalidation-mae:238.62514\n",
      "[7]\tvalidation-mae:236.71330\n",
      "[8]\tvalidation-mae:234.91794\n",
      "[9]\tvalidation-mae:233.14058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,513]\u001b[0m Trial 10 finished with value: 233.14810751508503 and parameters: {'booster': 'dart', 'colsample_bytree': 0.13370260690385866, 'eta': 0.016402750611731437, 'gamma': 0.5879067856210791, 'max_depth': 8, 'max_leaves': 25, 'alpha': 0.0007873923944244643, 'lambda': 1.1327386519348417}. Best is trial 10 with value: 233.14810751508503.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.52258\n",
      "[1]\tvalidation-mae:248.09761\n",
      "[2]\tvalidation-mae:243.80067\n",
      "[3]\tvalidation-mae:240.96433\n",
      "[4]\tvalidation-mae:238.50853\n",
      "[5]\tvalidation-mae:235.97293\n",
      "[6]\tvalidation-mae:233.54550\n",
      "[7]\tvalidation-mae:231.15527\n",
      "[8]\tvalidation-mae:228.89754\n",
      "[9]\tvalidation-mae:226.71072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:16,741]\u001b[0m Trial 11 finished with value: 226.71695008228195 and parameters: {'booster': 'dart', 'colsample_bytree': 0.012810559441392255, 'eta': 0.022839045981270574, 'gamma': 0.5345771979469582, 'max_depth': 8, 'max_leaves': 50, 'alpha': 0.0016513489379228637, 'lambda': 1.518941081085}. Best is trial 10 with value: 233.14810751508503.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:250.41058\n",
      "[1]\tvalidation-mae:246.06709\n",
      "[2]\tvalidation-mae:240.80879\n",
      "[3]\tvalidation-mae:237.27338\n",
      "[4]\tvalidation-mae:234.25638\n",
      "[5]\tvalidation-mae:231.16612\n",
      "[6]\tvalidation-mae:228.23932\n",
      "[7]\tvalidation-mae:225.41602\n",
      "[8]\tvalidation-mae:222.72786\n",
      "[9]\tvalidation-mae:220.25032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:17,317]\u001b[0m Trial 12 finished with value: 220.2589138782227 and parameters: {'booster': 'dart', 'colsample_bytree': 0.008646042567240317, 'eta': 0.03011524804208298, 'gamma': 0.6000096621185084, 'max_depth': 8, 'max_leaves': 49, 'alpha': 0.0017783514601667162, 'lambda': 0.8702200150786426}. Best is trial 10 with value: 233.14810751508503.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:252.69720\n",
      "[1]\tvalidation-mae:250.36079\n",
      "[2]\tvalidation-mae:247.29527\n",
      "[3]\tvalidation-mae:245.22147\n",
      "[4]\tvalidation-mae:243.40279\n",
      "[5]\tvalidation-mae:241.57823\n",
      "[6]\tvalidation-mae:239.81772\n",
      "[7]\tvalidation-mae:238.04079\n",
      "[8]\tvalidation-mae:236.35405\n",
      "[9]\tvalidation-mae:234.69740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:17,740]\u001b[0m Trial 13 finished with value: 234.70378496982994 and parameters: {'booster': 'dart', 'colsample_bytree': 0.017456199445925816, 'eta': 0.014901062528861475, 'gamma': 0.6171734213167471, 'max_depth': 7, 'max_leaves': 21, 'alpha': 0.008460481811159555, 'lambda': 0.030182457670580934}. Best is trial 13 with value: 234.70378496982994.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:239.35149\n",
      "[1]\tvalidation-mae:227.04033\n",
      "[2]\tvalidation-mae:213.83942\n",
      "[3]\tvalidation-mae:198.74040\n",
      "[4]\tvalidation-mae:190.79274\n",
      "[5]\tvalidation-mae:183.11829\n",
      "[6]\tvalidation-mae:177.01767\n",
      "[7]\tvalidation-mae:172.97804\n",
      "[8]\tvalidation-mae:168.68922\n",
      "[9]\tvalidation-mae:165.52843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:18,068]\u001b[0m Trial 14 finished with value: 165.53812397147558 and parameters: {'booster': 'dart', 'colsample_bytree': 0.1870113241588211, 'eta': 0.11533885995694225, 'gamma': 0.6668033987218592, 'max_depth': 7, 'max_leaves': 19, 'alpha': 0.032913208611693265, 'lambda': 0.01186552086650203}. Best is trial 13 with value: 234.70378496982994.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:18,172]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:235.58740\n",
      "[1]\tvalidation-mae:221.22157\n",
      "[2]\tvalidation-mae:206.45668\n",
      "[3]\tvalidation-mae:188.64699\n",
      "[4]\tvalidation-mae:179.03233\n",
      "[5]\tvalidation-mae:171.24886\n",
      "[6]\tvalidation-mae:167.02747\n",
      "[7]\tvalidation-mae:163.35515\n",
      "[8]\tvalidation-mae:159.24654\n",
      "[9]\tvalidation-mae:156.60039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:18,438]\u001b[0m Trial 16 finished with value: 156.6072408118486 and parameters: {'booster': 'dart', 'colsample_bytree': 0.1843876959054057, 'eta': 0.14759569302527883, 'gamma': 0.7062578274536828, 'max_depth': 4, 'max_leaves': 13, 'alpha': 0.0003809747602366577, 'lambda': 0.10107459830171373}. Best is trial 13 with value: 234.70378496982994.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:252.40985\n",
      "[1]\tvalidation-mae:249.93796\n",
      "[2]\tvalidation-mae:247.45653\n",
      "[3]\tvalidation-mae:244.67595\n",
      "[4]\tvalidation-mae:241.90695\n",
      "[5]\tvalidation-mae:240.86023\n",
      "[6]\tvalidation-mae:238.23689\n",
      "[7]\tvalidation-mae:235.68753\n",
      "[8]\tvalidation-mae:233.69235\n",
      "[9]\tvalidation-mae:231.74524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:18,725]\u001b[0m Trial 17 finished with value: 231.74437739989028 and parameters: {'booster': 'dart', 'colsample_bytree': 0.4056393444273032, 'eta': 0.016723401888589506, 'gamma': 0.004329338379553793, 'max_depth': 5, 'max_leaves': 28, 'alpha': 0.012524233422090221, 'lambda': 0.23618338264785524}. Best is trial 13 with value: 234.70378496982994.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:237.86353\n",
      "[1]\tvalidation-mae:224.74849\n",
      "[2]\tvalidation-mae:211.21916\n",
      "[3]\tvalidation-mae:202.77017\n",
      "[4]\tvalidation-mae:196.82817\n",
      "[5]\tvalidation-mae:191.15439\n",
      "[6]\tvalidation-mae:186.33005\n",
      "[7]\tvalidation-mae:182.76567\n",
      "[8]\tvalidation-mae:178.81988\n",
      "[9]\tvalidation-mae:176.36598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:19,055]\u001b[0m Trial 18 finished with value: 176.36094349972572 and parameters: {'booster': 'dart', 'colsample_bytree': 0.08793443237815254, 'eta': 0.12801535160286664, 'gamma': 0.4724671208979009, 'max_depth': 7, 'max_leaves': 23, 'alpha': 0.8651524785521447, 'lambda': 0.002611350704375609}. Best is trial 13 with value: 234.70378496982994.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:19,127]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:152.65645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:19,178]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.81189\n",
      "[1]\tvalidation-mae:254.58263\n",
      "[2]\tvalidation-mae:254.33891\n",
      "[3]\tvalidation-mae:254.07546\n",
      "[4]\tvalidation-mae:253.81163\n",
      "[5]\tvalidation-mae:253.71027\n",
      "[6]\tvalidation-mae:253.45319\n",
      "[7]\tvalidation-mae:253.19934\n",
      "[8]\tvalidation-mae:252.97305\n",
      "[9]\tvalidation-mae:252.74293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:19,385]\u001b[0m Trial 21 finished with value: 252.72462973121227 and parameters: {'booster': 'dart', 'colsample_bytree': 0.45445165705809715, 'eta': 0.0014947206168516684, 'gamma': 0.01612373111343296, 'max_depth': 5, 'max_leaves': 30, 'alpha': 0.012026587099244288, 'lambda': 0.25202094699264005}. Best is trial 21 with value: 252.72462973121227.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:244.01559\n",
      "[1]\tvalidation-mae:234.98126\n",
      "[2]\tvalidation-mae:224.59738\n",
      "[3]\tvalidation-mae:217.95264\n",
      "[4]\tvalidation-mae:212.94510\n",
      "[5]\tvalidation-mae:207.93269\n",
      "[6]\tvalidation-mae:203.45468\n",
      "[7]\tvalidation-mae:199.56294\n",
      "[8]\tvalidation-mae:195.63724\n",
      "[9]\tvalidation-mae:192.55420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:19,934]\u001b[0m Trial 22 finished with value: 192.56390565002744 and parameters: {'booster': 'dart', 'colsample_bytree': 0.12104135734321227, 'eta': 0.0772188599359801, 'gamma': 0.5571502068786501, 'max_depth': 7, 'max_leaves': 33, 'alpha': 0.0025698027979181737, 'lambda': 1.952365954662215}. Best is trial 21 with value: 252.72462973121227.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:20,077]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:240.69788\n",
      "[1]\tvalidation-mae:230.19733\n",
      "[2]\tvalidation-mae:221.61876\n",
      "[3]\tvalidation-mae:209.19983\n",
      "[4]\tvalidation-mae:197.23483\n",
      "[5]\tvalidation-mae:194.85391\n",
      "[6]\tvalidation-mae:185.39752\n",
      "[7]\tvalidation-mae:176.73910\n",
      "[8]\tvalidation-mae:171.76831\n",
      "[9]\tvalidation-mae:167.55502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:20,425]\u001b[0m Trial 24 finished with value: 167.5496434448711 and parameters: {'booster': 'dart', 'colsample_bytree': 0.44703965211129626, 'eta': 0.09260293532283156, 'gamma': 0.01297402586185995, 'max_depth': 6, 'max_leaves': 45, 'alpha': 0.0004700651470235227, 'lambda': 0.23982242630935677}. Best is trial 21 with value: 252.72462973121227.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:219.02824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:20,538]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.98463\n",
      "[1]\tvalidation-mae:254.91850\n",
      "[2]\tvalidation-mae:254.82571\n",
      "[3]\tvalidation-mae:254.75356\n",
      "[4]\tvalidation-mae:254.68272\n",
      "[5]\tvalidation-mae:254.60141\n",
      "[6]\tvalidation-mae:254.53130\n",
      "[7]\tvalidation-mae:254.46497\n",
      "[8]\tvalidation-mae:254.40184\n",
      "[9]\tvalidation-mae:254.33545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:21,026]\u001b[0m Trial 26 finished with value: 254.43115743280308 and parameters: {'booster': 'dart', 'colsample_bytree': 0.23738216558043013, 'eta': 0.00040779662161086454, 'gamma': 0.21939871799535107, 'max_depth': 5, 'max_leaves': 33, 'alpha': 0.059013668820586174, 'lambda': 0.005365176395834468}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:215.10941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:21,088]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.93733\n",
      "[1]\tvalidation-mae:254.82338\n",
      "[2]\tvalidation-mae:254.66504\n",
      "[3]\tvalidation-mae:254.54131\n",
      "[4]\tvalidation-mae:254.40697\n",
      "[5]\tvalidation-mae:254.27115\n",
      "[6]\tvalidation-mae:254.16676\n",
      "[7]\tvalidation-mae:254.05292\n",
      "[8]\tvalidation-mae:253.94244\n",
      "[9]\tvalidation-mae:253.82985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:21,496]\u001b[0m Trial 28 finished with value: 253.77070762479428 and parameters: {'booster': 'dart', 'colsample_bytree': 0.2466161538896719, 'eta': 0.0007012944327867525, 'gamma': 0.2326259389029187, 'max_depth': 3, 'max_leaves': 33, 'alpha': 0.06957638604598959, 'lambda': 0.007823816189683523}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:21,571]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:21,627]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.65878\n",
      "[1]\tvalidation-mae:254.26526\n",
      "[2]\tvalidation-mae:253.71400\n",
      "[3]\tvalidation-mae:253.28439\n",
      "[4]\tvalidation-mae:252.82089\n",
      "[5]\tvalidation-mae:252.35373\n",
      "[6]\tvalidation-mae:251.99638\n",
      "[7]\tvalidation-mae:251.60841\n",
      "[8]\tvalidation-mae:251.23306\n",
      "[9]\tvalidation-mae:250.85388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:21,859]\u001b[0m Trial 31 finished with value: 250.86341195831048 and parameters: {'booster': 'dart', 'colsample_bytree': 0.24479802709904797, 'eta': 0.0024477591207734293, 'gamma': 0.23186678272820732, 'max_depth': 3, 'max_leaves': 19, 'alpha': 0.0348110150033186, 'lambda': 0.03610994016017575}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:21,921]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:21,977]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:221.25668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:22,035]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:22,088]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:168.91376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:22,150]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:246.26411\n",
      "[1]\tvalidation-mae:239.38307\n",
      "[2]\tvalidation-mae:231.29627\n",
      "[3]\tvalidation-mae:222.80058\n",
      "[4]\tvalidation-mae:213.68414\n",
      "[5]\tvalidation-mae:207.76949\n",
      "[6]\tvalidation-mae:201.12776\n",
      "[7]\tvalidation-mae:194.97414\n",
      "[8]\tvalidation-mae:190.95892\n",
      "[9]\tvalidation-mae:187.57591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:22,479]\u001b[0m Trial 37 finished with value: 187.64947888096543 and parameters: {'booster': 'dart', 'colsample_bytree': 0.3844869675944664, 'eta': 0.057316318510889155, 'gamma': 0.3373164964460467, 'max_depth': 3, 'max_leaves': 35, 'alpha': 0.028380979386981746, 'lambda': 0.00022971523134461365}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:22,556]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:252.72070\n",
      "[1]\tvalidation-mae:250.46768\n",
      "[2]\tvalidation-mae:248.33462\n",
      "[3]\tvalidation-mae:246.32408\n",
      "[4]\tvalidation-mae:244.42981\n",
      "[5]\tvalidation-mae:242.64239\n",
      "[6]\tvalidation-mae:240.93747\n",
      "[7]\tvalidation-mae:239.28244\n",
      "[8]\tvalidation-mae:237.67992\n",
      "[9]\tvalidation-mae:236.11581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:22,812]\u001b[0m Trial 39 finished with value: 236.1108063631377 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.60855031039147, 'eta': 0.0013685086769823877, 'gamma': 0.18692143107731501, 'max_depth': 4, 'max_leaves': 20, 'alpha': 0.199507937309336, 'lambda': 4.962427441816074e-05}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:22,927]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:22] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:22,988]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:23,044]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:254.51880\n",
      "[1]\tvalidation-mae:253.99153\n",
      "[2]\tvalidation-mae:253.46587\n",
      "[3]\tvalidation-mae:252.94469\n",
      "[4]\tvalidation-mae:252.42572\n",
      "[5]\tvalidation-mae:251.91191\n",
      "[6]\tvalidation-mae:251.40166\n",
      "[7]\tvalidation-mae:250.89755\n",
      "[8]\tvalidation-mae:250.39960\n",
      "[9]\tvalidation-mae:249.91000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:23,294]\u001b[0m Trial 43 finished with value: 249.90784421283598 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.5461289310259283, 'eta': 0.00030958819503464313, 'gamma': 0.2805682750686699, 'max_depth': 4, 'max_leaves': 8, 'alpha': 0.05428093282843006, 'lambda': 1.124343037246199e-06}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:166.05165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:23,360]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:23,434]\u001b[0m Trial 45 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:168.68013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:23,518]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:23,600]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:239.25516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:23,682]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:23,769]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:23,823]\u001b[0m Trial 50 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[12:47:23] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:252.47888\n",
      "[1]\tvalidation-mae:250.00607\n",
      "[2]\tvalidation-mae:247.68027\n",
      "[3]\tvalidation-mae:245.50139\n",
      "[4]\tvalidation-mae:243.46420\n",
      "[5]\tvalidation-mae:241.54475\n",
      "[6]\tvalidation-mae:239.69505\n",
      "[7]\tvalidation-mae:237.90858\n",
      "[8]\tvalidation-mae:236.17389\n",
      "[9]\tvalidation-mae:234.48999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:24,018]\u001b[0m Trial 51 finished with value: 234.48436642896326 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.6496973847784628, 'eta': 0.0015144098876187528, 'gamma': 0.19349201089245432, 'max_depth': 4, 'max_leaves': 18, 'alpha': 0.2745309351471082, 'lambda': 5.021479226557882e-06}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:251.23444\n",
      "[1]\tvalidation-mae:247.70810\n",
      "[2]\tvalidation-mae:244.50038\n",
      "[3]\tvalidation-mae:241.58615\n",
      "[4]\tvalidation-mae:238.84140\n",
      "[5]\tvalidation-mae:236.22508\n",
      "[6]\tvalidation-mae:233.71845\n",
      "[7]\tvalidation-mae:231.31810\n",
      "[8]\tvalidation-mae:229.02454\n",
      "[9]\tvalidation-mae:226.82796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:24,308]\u001b[0m Trial 52 finished with value: 226.82775644541962 and parameters: {'booster': 'gblinear', 'colsample_bytree': 0.5566845699106611, 'eta': 0.002284454006265693, 'gamma': 0.2619772086436061, 'max_depth': 4, 'max_leaves': 21, 'alpha': 0.005273620625543947, 'lambda': 0.08400473640652761}. Best is trial 26 with value: 254.43115743280308.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:24,393]\u001b[0m Trial 53 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:24,484]\u001b[0m Trial 54 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:47:24] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\tvalidation-mae:183.22925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:24,546]\u001b[0m Trial 55 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:24,626]\u001b[0m Trial 56 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:24,702]\u001b[0m Trial 57 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:24,755]\u001b[0m Trial 58 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:255.04071\n",
      "[1]\tvalidation-mae:255.03139\n",
      "[2]\tvalidation-mae:255.02141\n",
      "[3]\tvalidation-mae:255.01360\n",
      "[4]\tvalidation-mae:255.00430\n",
      "[5]\tvalidation-mae:254.99506\n",
      "[6]\tvalidation-mae:254.98671\n",
      "[7]\tvalidation-mae:254.97861\n",
      "[8]\tvalidation-mae:254.97055\n",
      "[9]\tvalidation-mae:254.96266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:24,993]\u001b[0m Trial 59 finished with value: 254.54854635216677 and parameters: {'booster': 'dart', 'colsample_bytree': 0.6694010125888245, 'eta': 4.8781693920985254e-05, 'gamma': 0.1883994647052204, 'max_depth': 3, 'max_leaves': 23, 'alpha': 9.801925286079314e-07, 'lambda': 0.05802906174357909}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:25,079]\u001b[0m Trial 60 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.53134\n",
      "[1]\tvalidation-mae:253.91237\n",
      "[2]\tvalidation-mae:253.35815\n",
      "[3]\tvalidation-mae:252.84651\n",
      "[4]\tvalidation-mae:252.31961\n",
      "[5]\tvalidation-mae:251.79912\n",
      "[6]\tvalidation-mae:251.29134\n",
      "[7]\tvalidation-mae:250.78487\n",
      "[8]\tvalidation-mae:250.28305\n",
      "[9]\tvalidation-mae:249.79631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:25,381]\u001b[0m Trial 61 finished with value: 249.7789358200768 and parameters: {'booster': 'dart', 'colsample_bytree': 0.7101546874147677, 'eta': 0.003004520163013718, 'gamma': 0.19461067666219606, 'max_depth': 4, 'max_leaves': 30, 'alpha': 1.2723838936293059e-06, 'lambda': 0.12810906351802373}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:241.17630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:25,480]\u001b[0m Trial 62 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:248.11168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:25,555]\u001b[0m Trial 63 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:164.49046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:25,615]\u001b[0m Trial 64 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:239.33803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:25,674]\u001b[0m Trial 65 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:25,740]\u001b[0m Trial 66 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:26,031]\u001b[0m Trial 67 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:26,122]\u001b[0m Trial 68 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:26,196]\u001b[0m Trial 69 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:26,290]\u001b[0m Trial 70 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.79900\n",
      "[1]\tvalidation-mae:254.54829\n",
      "[2]\tvalidation-mae:254.27318\n",
      "[3]\tvalidation-mae:254.02708\n",
      "[4]\tvalidation-mae:253.63711\n",
      "[5]\tvalidation-mae:253.33003\n",
      "[6]\tvalidation-mae:253.08191\n",
      "[7]\tvalidation-mae:252.83315\n",
      "[8]\tvalidation-mae:252.61156\n",
      "[9]\tvalidation-mae:252.37083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:26,844]\u001b[0m Trial 71 finished with value: 252.42128359846407 and parameters: {'booster': 'dart', 'colsample_bytree': 0.6005549841163182, 'eta': 0.0014584672824070914, 'gamma': 0.18382813257360642, 'max_depth': 5, 'max_leaves': 21, 'alpha': 0.16587747143977094, 'lambda': 0.009659497530801121}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.93318\n",
      "[1]\tvalidation-mae:254.78906\n",
      "[2]\tvalidation-mae:254.66150\n",
      "[3]\tvalidation-mae:254.54703\n",
      "[4]\tvalidation-mae:254.38919\n",
      "[5]\tvalidation-mae:254.24661\n",
      "[6]\tvalidation-mae:254.13142\n",
      "[7]\tvalidation-mae:254.01483\n",
      "[8]\tvalidation-mae:253.90097\n",
      "[9]\tvalidation-mae:253.78850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:27,657]\u001b[0m Trial 72 finished with value: 253.78277564454197 and parameters: {'booster': 'dart', 'colsample_bytree': 0.6454649769620846, 'eta': 0.000675900963850905, 'gamma': 0.178663453919822, 'max_depth': 5, 'max_leaves': 27, 'alpha': 0.04514990757869015, 'lambda': 0.008348284486348927}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:27,754]\u001b[0m Trial 73 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:27,815]\u001b[0m Trial 74 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:242.95200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:27,882]\u001b[0m Trial 75 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:27,937]\u001b[0m Trial 76 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:27,989]\u001b[0m Trial 77 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:28,044]\u001b[0m Trial 78 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.63121\n",
      "[1]\tvalidation-mae:254.22517\n",
      "[2]\tvalidation-mae:253.79491\n",
      "[3]\tvalidation-mae:253.32945\n",
      "[4]\tvalidation-mae:252.86417\n",
      "[5]\tvalidation-mae:252.68613\n",
      "[6]\tvalidation-mae:252.23377\n",
      "[7]\tvalidation-mae:251.78778\n",
      "[8]\tvalidation-mae:251.39154\n",
      "[9]\tvalidation-mae:250.98923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:28,276]\u001b[0m Trial 79 finished with value: 251.0082281952825 and parameters: {'booster': 'dart', 'colsample_bytree': 0.418015787253383, 'eta': 0.0026468283659037938, 'gamma': 0.17318631251046415, 'max_depth': 5, 'max_leaves': 23, 'alpha': 1.0099828883082265e-08, 'lambda': 0.03977544283765764}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:28,354]\u001b[0m Trial 80 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.39053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:28,420]\u001b[0m Trial 81 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.95605\n",
      "[1]\tvalidation-mae:254.87013\n",
      "[2]\tvalidation-mae:254.76524\n",
      "[3]\tvalidation-mae:254.67087\n",
      "[4]\tvalidation-mae:254.57767\n",
      "[5]\tvalidation-mae:254.47055\n",
      "[6]\tvalidation-mae:254.37869\n",
      "[7]\tvalidation-mae:254.28723\n",
      "[8]\tvalidation-mae:254.20436\n",
      "[9]\tvalidation-mae:254.12030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:28,945]\u001b[0m Trial 82 finished with value: 254.1947339550192 and parameters: {'booster': 'dart', 'colsample_bytree': 0.31826139640376777, 'eta': 0.0005371692468995047, 'gamma': 0.24157106964537703, 'max_depth': 6, 'max_leaves': 24, 'alpha': 0.05505430018663336, 'lambda': 0.005767417312910815}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:29,016]\u001b[0m Trial 83 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:29,075]\u001b[0m Trial 84 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:29,172]\u001b[0m Trial 85 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:29,233]\u001b[0m Trial 86 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.48022\n",
      "[1]\tvalidation-mae:253.96219\n",
      "[2]\tvalidation-mae:253.32671\n",
      "[3]\tvalidation-mae:252.75398\n",
      "[4]\tvalidation-mae:252.20393\n",
      "[5]\tvalidation-mae:251.56644\n",
      "[6]\tvalidation-mae:251.01180\n",
      "[7]\tvalidation-mae:250.46645\n",
      "[8]\tvalidation-mae:249.98171\n",
      "[9]\tvalidation-mae:249.49225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:29,636]\u001b[0m Trial 87 finished with value: 249.49204607789358 and parameters: {'booster': 'dart', 'colsample_bytree': 0.3169503257605775, 'eta': 0.0032626101590393424, 'gamma': 0.1741891276211291, 'max_depth': 7, 'max_leaves': 28, 'alpha': 0.0726311674266008, 'lambda': 0.013505462257396864}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:248.60928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:29,732]\u001b[0m Trial 88 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:29,861]\u001b[0m Trial 89 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:29,928]\u001b[0m Trial 90 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.69289\n",
      "[1]\tvalidation-mae:254.35425\n",
      "[2]\tvalidation-mae:254.02331\n",
      "[3]\tvalidation-mae:253.65153\n",
      "[4]\tvalidation-mae:253.07117\n",
      "[5]\tvalidation-mae:252.92227\n",
      "[6]\tvalidation-mae:252.54835\n",
      "[7]\tvalidation-mae:252.17364\n",
      "[8]\tvalidation-mae:251.84256\n",
      "[9]\tvalidation-mae:251.49863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:30,303]\u001b[0m Trial 91 finished with value: 251.4947888096544 and parameters: {'booster': 'dart', 'colsample_bytree': 0.5337031015526524, 'eta': 0.0022082522397952695, 'gamma': 0.2741664609590848, 'max_depth': 5, 'max_leaves': 23, 'alpha': 0.054914272369285415, 'lambda': 0.094652952168508}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:251.61485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:30,422]\u001b[0m Trial 92 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:30,502]\u001b[0m Trial 93 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.82234\n",
      "[1]\tvalidation-mae:254.61064\n",
      "[2]\tvalidation-mae:254.35043\n",
      "[3]\tvalidation-mae:254.11859\n",
      "[4]\tvalidation-mae:253.88930\n",
      "[5]\tvalidation-mae:253.62736\n",
      "[6]\tvalidation-mae:253.40022\n",
      "[7]\tvalidation-mae:253.17679\n",
      "[8]\tvalidation-mae:252.97533\n",
      "[9]\tvalidation-mae:252.77255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:30,804]\u001b[0m Trial 94 finished with value: 252.7460230389468 and parameters: {'booster': 'dart', 'colsample_bytree': 0.3518568562480109, 'eta': 0.001313647111257916, 'gamma': 0.3064316838549543, 'max_depth': 5, 'max_leaves': 19, 'alpha': 0.2689197135602547, 'lambda': 0.034816284744779116}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:240.76273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:30,941]\u001b[0m Trial 95 pruned. Trial was pruned at iteration 0.\u001b[0m\n",
      "\u001b[32m[I 2022-04-07 12:47:31,570]\u001b[0m Trial 96 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:252.38162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:31,667]\u001b[0m Trial 97 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:244.79713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:31,767]\u001b[0m Trial 98 pruned. Trial was pruned at iteration 0.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation-mae:254.87318\n",
      "[1]\tvalidation-mae:254.70602\n",
      "[2]\tvalidation-mae:254.54275\n",
      "[3]\tvalidation-mae:254.35919\n",
      "[4]\tvalidation-mae:254.07248\n",
      "[5]\tvalidation-mae:253.99863\n",
      "[6]\tvalidation-mae:253.81337\n",
      "[7]\tvalidation-mae:253.62761\n",
      "[8]\tvalidation-mae:253.46304\n",
      "[9]\tvalidation-mae:253.29166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 12:47:32,502]\u001b[0m Trial 99 finished with value: 253.3368074602304 and parameters: {'booster': 'dart', 'colsample_bytree': 0.45459283718862975, 'eta': 0.0010870453353754679, 'gamma': 0.32535204473476326, 'max_depth': 5, 'max_leaves': 16, 'alpha': 0.07089230759616938, 'lambda': 0.3013753502048186}. Best is trial 59 with value: 254.54854635216677.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenTrial(number=59, values=[254.54854635216677], datetime_start=datetime.datetime(2022, 4, 7, 12, 47, 24, 756049), datetime_complete=datetime.datetime(2022, 4, 7, 12, 47, 24, 992663), params={'booster': 'dart', 'colsample_bytree': 0.6694010125888245, 'eta': 4.8781693920985254e-05, 'gamma': 0.1883994647052204, 'max_depth': 3, 'max_leaves': 23, 'alpha': 9.801925286079314e-07, 'lambda': 0.05802906174357909}, distributions={'booster': CategoricalDistribution(choices=('gbtree', 'gblinear', 'dart')), 'colsample_bytree': UniformDistribution(high=1.0, low=0.0), 'eta': UniformDistribution(high=1.0, low=1e-08), 'gamma': UniformDistribution(high=1.0, low=1e-08), 'max_depth': IntUniformDistribution(high=9, low=1, step=1), 'max_leaves': IntUniformDistribution(high=50, low=0, step=1), 'alpha': LogUniformDistribution(high=2.0, low=1e-08), 'lambda': LogUniformDistribution(high=2.0, low=1e-08)}, user_attrs={}, system_attrs={}, intermediate_values={0: 255.04071, 1: 255.031387, 2: 255.021408, 3: 255.013596, 4: 255.004303, 5: 254.995056, 6: 254.98671, 7: 254.978607, 8: 254.970551, 9: 254.962662}, trial_id=59, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean abs err : 254.54854635216677\n",
      "best params : {'booster': 'dart', 'colsample_bytree': 0.6694010125888245, 'eta': 4.8781693920985254e-05, 'gamma': 0.1883994647052204, 'max_depth': 3, 'max_leaves': 23, 'alpha': 9.801925286079314e-07, 'lambda': 0.05802906174357909}\n"
     ]
    }
   ],
   "source": [
    "trial = study.best_trial\n",
    "print(f'mean abs err : {trial.value}')\n",
    "print('best params :', trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models \n",
    "xgb = xgboost.XGBRegressor(booster='gbtree', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=32, \n",
    "                            reg_alpha=1.6666666666666667, reg_lambda =1.9791666666666667, subsample_freq=0.75, tree_method='auto', verbose=-1)\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7, verbose=-1)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56, verbose=-1)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56, verbose=-1)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7,  verbose=-1)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)],)\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model_2 = StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "\n",
    "liste_model = [xgb, model_lgbm, model_lgbm_2, model_lgbm_3, model_lgbm_4, VC, stack_model, stack_model_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:32:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"subsample_freq\", \"verbose\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "train data 0.986720359083927\n",
      "test data 0.8967965326020695\n"
     ]
    }
   ],
   "source": [
    "# Meilleur model : \n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "logfunc = FunctionTransformer(func=np.log1p, inverse_func=np.exp, check_inverse=False)\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(StandardScaler(with_mean=True, with_std=False), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_selected = make_pipeline(preprocessor,  xgb)\n",
    "model_selected.fit(train_data_all_X , train_data_all_y)\n",
    "print('train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('test data', model_selected.score(test_data_all_X, test_data_all_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model stacking\n",
      "r2 score train data 0.986720359083927\n",
      "r2 score test data 0.8967965326020695\n",
      "mean squared error 4793.1408218943025\n",
      "median abs error 32.69251251220703\n",
      "mean abs error 48.643447046910445\n"
     ]
    }
   ],
   "source": [
    "print(f'model stacking') \n",
    "\n",
    "\n",
    "print('r2 score train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('r2 score test data', model_selected.score(test_data_all_X, test_data_all_y))\n",
    "y_pred = model_selected.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A ne pas touché ! Model saved avec le meilleur score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:48:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:29] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:31] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:32] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:33] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:34] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:35] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:39] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[13:49:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"colsample_bytree\", \"gamma\", \"max_depth\", \"max_leaves\", \"subsample_freq\", \"tree_method\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('maxabsscaler',\n",
       "                                                                   MaxAbsScaler())]),\n",
       "                                                  ['temp', 'atemp', 'humidity',\n",
       "                                                   'windspeed', 'day',\n",
       "                                                   'hour']),\n",
       "                                                 ('pipeline-2',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer()),\n",
       "                                                                  ('standardscaler',\n",
       "                                                                   StandardScaler(with_mean=False)),\n",
       "                                                                  ('onehotencode...\n",
       "                                                                                           max_bin=725,\n",
       "                                                                                           max_depth=8,\n",
       "                                                                                           random_state=2,\n",
       "                                                                                           reg_alpha=0,\n",
       "                                                                                           reg_lambda=0.68596,\n",
       "                                                                                           subsample_freq=56)),\n",
       "                                                                            ('lgbm3',\n",
       "                                                                             LGBMRegressor(max_bin=1023,\n",
       "                                                                                           min_data_in_leaf=100,\n",
       "                                                                                           random_state=2,\n",
       "                                                                                           reg_alpha=0,\n",
       "                                                                                           subsample_freq=56)),\n",
       "                                                                            ('lgbm4',\n",
       "                                                                             LGBMRegressor(colsample_bytree=0.469,\n",
       "                                                                                           learning_rate=0.1239599,\n",
       "                                                                                           max_bin=1023,\n",
       "                                                                                           max_depth=8,\n",
       "                                                                                           random_state=2,\n",
       "                                                                                           reg_alpha=0,\n",
       "                                                                                           reg_lambda=1,\n",
       "                                                                                           subsample_freq=7))]))]))])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Meilleur model : train data 0.9534144126106945\n",
    "#                  test data 0.9222952589076324 ou 92.24 avec StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "# Preprocessing et model xgboost :0.9056841627368329(8s)\n",
    "\n",
    "# All models \n",
    "\n",
    "xgb = xgboost.XGBRegressor(booster='gblinear', colsample_bytree = 0.493659, eta = 0.28, gamma=0,max_depth = 7, max_leaves=0, n_estimators =100, objective='reg:squarederror', random_state=2, \n",
    "                            reg_alpha=0.22936408146810935, reg_lambda = 0.03113496231279571, subsample_freq=0.75, tree_method='auto')\n",
    "\n",
    "model_lgbm =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7395, learning_rate=0.12, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "model_lgbm_2 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.7385, learning_rate=0.07, max_bin=725, max_depth=8, reg_lambda=0.68596,reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_3 = lightgbm.LGBMRegressor(random_state=2, min_data_in_leaf=100, max_bin=1023, reg_alpha=0, subsample_freq=56)\n",
    "\n",
    "model_lgbm_4 =  lightgbm.LGBMRegressor(random_state=2, colsample_bytree=0.469, learning_rate=0.1239599, max_bin=1023, max_depth=8, reg_lambda=1,reg_alpha=0, subsample_freq=7)\n",
    "\n",
    "VC = VotingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3), ('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model = StackingRegressor([('xgboos' , xgb),('lgbm' , model_lgbm), ('lgbm2' , model_lgbm_2),('lgbm3' , model_lgbm_3),('lgbm4' , model_lgbm_4)])\n",
    "\n",
    "stack_model_2 = StackingRegressor( [('1' , model_lgbm), ('3', xgb), ('4', VC)])\n",
    "\n",
    "numerical_features = ['temp','atemp','humidity','windspeed','day','hour']\n",
    "encode_column = ['season','workingday','weather','holiday','hour','year']\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(), MaxAbsScaler())\n",
    "encode_pipeline = make_pipeline(SimpleImputer(), StandardScaler(with_mean=False, with_std=True), OneHotEncoder(handle_unknown = 'ignore'))\n",
    "\n",
    "\n",
    "preprocessor = make_column_transformer((numerical_pipeline, numerical_features),(encode_pipeline , encode_column))\n",
    "\n",
    "\n",
    "model_selected = make_pipeline(preprocessor,  stack_model_2)\n",
    "model_selected.fit(train_data_all_X , train_data_all_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model selectionnée\n",
      "r2 score train data 0.9548144146357441\n",
      "r2 score test data 0.9224655149630334\n",
      "mean squared error 3600.980807188359\n",
      "median abs error 26.733229801945807\n",
      "mean abs error 41.12271104743648\n"
     ]
    }
   ],
   "source": [
    "print(f'model selectionnée') \n",
    "\n",
    "\n",
    "print('r2 score train data', model_selected.score(train_data_all_X, train_data_all_y))\n",
    "print('r2 score test data', model_selected.score(test_data_all_X, test_data_all_y))\n",
    "y_pred = model_selected.predict(test_data_all_X)\n",
    "print('mean squared error', mean_squared_error(test_data_all_y, y_pred ))\n",
    "print('median abs error', median_absolute_error(test_data_all_y, y_pred))\n",
    "print('mean abs error' , mean_absolute_error(test_data_all_y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'stacking.pkl'\n",
    "pickle.dump(model_xgboost, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45357ead6f4da03ac2d24128fc463b8aec036f33d9539f6445fb7785aa61cc64"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('envIA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
